# 梯度下降法

## 相关定义

- 回归在数学上来说是给定一个点集，能够用一条曲线去拟合之，如果这个曲线是一条直线，那就被称为线性回归，如果曲线是一条二次曲线，就被称为二次回归
- 拟合的函数（或者称为假设或者模型），一般写做 y = h(x)
- 导数是函数f(x)在x轴上某一点处沿着x轴正方向的变化率/变化趋势。
- 导数与偏导数本质是一致的，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。 

## 参考资料

- [网易公开课](http://open.163.com/movie/2008/1/B/O/M6SGF6VB4_M6SGHJ9BO.html)
- <https://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html>
- [电驴资源](http://www.verycd.com/entries/531603/#163)
- <https://ctmakro.github.io/site/on_learning/gd.html>
- <https://www.cnblogs.com/pinard/p/5970503.html>
- [什么是全导数](https://www.zhihu.com/question/26966355/answer/154857139)
- [最小二乘法的本质](https://www.zhihu.com/question/37031188)
